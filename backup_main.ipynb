{"cells":[{"cell_type":"markdown","metadata":{"id":"vobP7ahXP1EG"},"source":["# Graph-Learning-Based Recommender System on MovieLens\n","\n","### Group 9\n","\n","- AGARWAL, Sahil\n","- WEI, Yuanjing\n","- ZHANG, Yujun yzhanglo@connect.ust.hk\n","\n","Group project of COMP4222@HKUST in 2022 Fall."]},{"cell_type":"markdown","metadata":{"id":"QpTqYz7qYJbP"},"source":["# 1 Environment Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603},"executionInfo":{"elapsed":30299,"status":"error","timestamp":1667642237984,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"MXxR7IpwI83o","outputId":"9933f848-12fa-442f-afe4-0d95a20a464e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/data/yzhanglo/4222project\n","backup_main.ipynb  \u001b[0m\u001b[38;5;27mLightGCN\u001b[0m/      \u001b[38;5;27mml-latest-small\u001b[0m/  requirements.txt\n","\u001b[38;5;27mcomp4222\u001b[0m/          lightgcn.yaml  movielens.ipynb\n","\u001b[38;5;27mKGAT_folder\u001b[0m/       main.ipynb     \u001b[38;5;9mmovielens.zip\u001b[0m\n","LICENSE            \u001b[38;5;27mml-100k\u001b[0m/       \u001b[38;5;27mrecommenders\u001b[0m/\n"]}],"source":["# change the path in the following\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd '/content/drive/MyDrive/4222Group9'\n","except:\n","    %cd '/data/yzhanglo/4222project'\n","\n","import comp4222\n","import recommenders\n","%pwd\n","%ls"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1667642237987,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"7RuvfqktV-jm"},"outputs":[{"name":"stderr","output_type":"stream","text":["/data/yzhanglo/miniconda/envs/cu101/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2022-11-21 17:14:27.882965: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from tensorboardX import SummaryWriter\n","\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","# easier to print by putting variable as a single line\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# make matplotlib figures appear inline in the notebook rather than in a new window.\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# Add some convenience functions to Pandas DataFrame.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = '{:.2f}'.format\n","def mask(df, key, function):\n","  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n","  return df[function(df[key])]\n","\n","def flatten_cols(df):\n","  df.columns = [' '.join(col).strip() for col in df.columns.values]\n","  return df\n","\n","pd.DataFrame.mask = mask\n","pd.DataFrame.flatten_cols = flatten_cols\n","\n","# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1667642237988,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"zLFdbrlyOWiT"},"outputs":[{"name":"stdout","output_type":"stream","text":["hahaha\n"]}],"source":["# Testing the module functionality\n","from comp4222 import b\n","comp4222.b.ok()"]},{"cell_type":"markdown","metadata":{"id":"dkgjzCmnWXUw"},"source":["# 2 MovieLens\n"]},{"cell_type":"markdown","metadata":{"id":"Mq_PrL0LZrGa"},"source":["We're using ml-latest-small from MovieLens. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018. The readme.md is avaliable [here](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)."]},{"cell_type":"markdown","metadata":{"id":"g_XRmlt-yMUu"},"source":["## Data Loading for ml-latest-small"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237989,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"Vt7THxaKWZDv"},"outputs":[],"source":["# Download MovieLens data.\n","dataset_name = \"ml-latest-small\"\n","from urllib.request import urlretrieve\n","import zipfile\n","urlretrieve(f\"https://files.grouplens.org/datasets/movielens/{dataset_name}.zip\", \"movielens.zip\")\n","zipfile.ZipFile(\"movielens.zip\", \"r\").extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1667642237989,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"vhrmXmjSh5jf"},"outputs":[],"source":["movies = pd.read_csv(f\"{dataset_name}/movies.csv\")\n","genre_cols = [\n","    \"(no genres listed)\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n","    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n","    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n","]\n","movies"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237990,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"HipV8C2ziAUh"},"outputs":[],"source":["tags = pd.read_csv(f\"{dataset_name}/tags.csv\")\n","tags"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237990,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"RF0KOsiadLM1"},"outputs":[],"source":["ratings = pd.read_csv(f\"{dataset_name}/ratings.csv\")\n","ratings"]},{"cell_type":"markdown","metadata":{"id":"OzxFHwzLy2vU"},"source":["# 5 Backup Models Definition"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":473,"status":"error","timestamp":1667642260421,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"U0Q4UEcMy6g6","outputId":"62577a20-91f8-43cf-8ea8-5d9138fa711a"},"outputs":[],"source":["from recommenders.utils.timer import Timer\n","from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n","from recommenders.utils.constants import (\n","    COL_DICT,\n","    DEFAULT_K,\n","    DEFAULT_USER_COL,\n","    DEFAULT_ITEM_COL,\n","    DEFAULT_RATING_COL,\n","    DEFAULT_PREDICTION_COL,\n","    DEFAULT_TIMESTAMP_COL,\n","    SEED,\n",")\n","\n","# Helpers\n","import os\n","from tempfile import TemporaryDirectory\n","tmp_dir = TemporaryDirectory()\n","TRAIN_FILE = os.path.join(tmp_dir.name, \"df_train.csv\")\n","TEST_FILE = os.path.join(tmp_dir.name, \"df_test.csv\")\n","\n","from recommenders.evaluation.python_evaluation import (\n","    map_at_k,\n","    ndcg_at_k,\n","    precision_at_k,\n","    recall_at_k,\n",")\n","def ranking_metrics_python(test, predictions, k=DEFAULT_K):\n","    return {\n","        \"MAP\": map_at_k(test, predictions, k=k, **COL_DICT),\n","        \"nDCG@k\": ndcg_at_k(test, predictions, k=k, **COL_DICT),\n","        \"Precision@k\": precision_at_k(test, predictions, k=k, **COL_DICT),\n","        \"Recall@k\": recall_at_k(test, predictions, k=k, **COL_DICT),\n","    }\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":6,"status":"error","timestamp":1667642261027,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"kBSE8h2MzB16","outputId":"cfd4e3c0-5af3-4a2d-f1ea-dddfcadbd897"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/11/21 17:14:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","22/11/21 17:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x7f43fb0b6df0>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from recommenders.utils.spark_utils import start_or_get_spark\n","spark = start_or_get_spark(\"PySpark\", memory=\"32g\")\n","spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n","\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_stratified_split\n","\n","# fix random seeds to make sure out runs are reproducible\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["### ALS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["als_params = {\n","    \"rank\": 10,\n","    \"maxIter\": 20,\n","    \"implicitPrefs\": False,\n","    \"alpha\": 0.1,\n","    \"regParam\": 0.05,\n","    \"coldStartStrategy\": \"drop\",\n","    \"nonnegative\": False,\n","    \"userCol\": DEFAULT_USER_COL,\n","    \"itemCol\": DEFAULT_ITEM_COL,\n","    \"ratingCol\": DEFAULT_RATING_COL,\n","}"]},{"cell_type":"markdown","metadata":{"id":"M5QhzXoRr0b6"},"source":["### NCF"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1667642261028,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"28jUVetSy9As"},"outputs":[],"source":["from recommenders.models.ncf.ncf_singlenode import NCF\n","from recommenders.models.ncf.dataset import Dataset as NCFDataset\n","\n","def prepare_training_ncf(df_train, df_test):\n","    #df_train.sort_values([\"userID\"], axis=0, ascending=[True], inplace=True)\n","    #df_test.sort_values([\"userID\"], axis=0, ascending=[True], inplace=True)\n","    train = df_train.sort_values([\"userID\"], axis=0, ascending=[True])\n","    test = df_test.sort_values([\"userID\"], axis=0, ascending=[True])\n","    test = test[df_test[\"userID\"].isin(train[\"userID\"].unique())]\n","    test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n","    train.to_csv(TRAIN_FILE, index=False)\n","    test.to_csv(TEST_FILE, index=False)\n","    return NCFDataset(\n","        train_file=TRAIN_FILE,\n","        col_user=DEFAULT_USER_COL,\n","        col_item=DEFAULT_ITEM_COL,\n","        col_rating=DEFAULT_RATING_COL,\n","        seed=SEED,\n","    )\n","\n","\n","def train_ncf(params, data):\n","    model = NCF(n_users=data.n_users, n_items=data.n_items, **params)\n","    with Timer() as t:\n","        model.fit(data)\n","    return model, t\n","\n","\n","def recommend_k_ncf(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n","    with Timer() as t:\n","        users, items, preds = [], [], []\n","        item = list(train[DEFAULT_ITEM_COL].unique())\n","        for user in train[DEFAULT_USER_COL].unique():\n","            user = [user] * len(item)\n","            users.extend(user)\n","            items.extend(item)\n","            preds.extend(list(model.predict(user, item, is_list=True)))\n","        topk_scores = pd.DataFrame(\n","            data={\n","                DEFAULT_USER_COL: users,\n","                DEFAULT_ITEM_COL: items,\n","                DEFAULT_PREDICTION_COL: preds,\n","            }\n","        )\n","        merged = pd.merge(\n","            train, topk_scores, on=[DEFAULT_USER_COL, DEFAULT_ITEM_COL], how=\"outer\"\n","        )\n","        topk_scores = merged[merged[DEFAULT_RATING_COL].isnull()].drop(\n","            DEFAULT_RATING_COL, axis=1\n","        )\n","    # Remove temp files\n","    return topk_scores, t\n","\n","ncf_params = {\n","    \"model_type\": \"NeuMF\",\n","    \"n_factors\": 4,\n","    \"layer_sizes\": [16, 8, 4],\n","    \"n_epochs\": 20,\n","    \"batch_size\": 1024,\n","    \"learning_rate\": 1e-3,\n","    \"verbose\": 10\n","}"]},{"cell_type":"markdown","metadata":{"id":"pIDpds_1tSxY"},"source":["### KGAT"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1667642261029,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"juTZLU9ktSxZ"},"outputs":[],"source":["#%pip install easydict\n","import os\n","import sys\n","import random\n","from time import time\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from KGAT_folder.KGAT import KGAT\n","from KGAT_folder.log_helper import *\n","from KGAT_folder.parser_kgat import *\n","from KGAT_folder.metrics import *\n","from KGAT_folder.model_helper import *\n","from KGAT_folder.loader_kgat import DataLoaderKGAT\n","\n","\n","def train_kgat():\n","    args = parse_kgat_args()\n","    #train(args)\n","    time0 = time()\n","    model, data, Ks, device = train(args)\n","    time1 = time()\n","    t = time1-time0\n","    return model, data, Ks, device, t\n","\n","def evaluate_kgat(model, dataloader, Ks, device):\n","    test_batch_size = dataloader.test_batch_size\n","    train_user_dict = dataloader.train_user_dict\n","    test_user_dict = dataloader.test_user_dict\n","\n","    model.eval()\n","\n","    user_ids = list(test_user_dict.keys())\n","    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n","    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n","\n","    n_items = dataloader.n_items\n","    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n","\n","    cf_scores = []\n","    metric_names = ['precision', 'recall', 'ndcg']\n","    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n","\n","    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n","        for batch_user_ids in user_ids_batches:\n","            batch_user_ids = batch_user_ids.to(device)\n","\n","            with torch.no_grad():\n","                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n","\n","            batch_scores = batch_scores.cpu()\n","            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n","\n","            cf_scores.append(batch_scores.numpy())\n","            for k in Ks:\n","                for m in metric_names:\n","                    metrics_dict[k][m].append(batch_metrics[k][m])\n","            pbar.update(1)\n","\n","    cf_scores = np.concatenate(cf_scores, axis=0)\n","    for k in Ks:\n","        for m in metric_names:\n","            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n","    return cf_scores, metrics_dict\n","\n","\n","def train(args):\n","    # seed\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.cuda.manual_seed_all(args.seed)\n","\n","    #log_save_id = create_log_id(args.save_dir)\n","    #logging_config(folder=args.save_dir, name='log{:d}'.format(log_save_id), no_console=False)\n","    #logging.info(args)\n","\n","    # GPU / CPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # load data\n","    data = DataLoaderKGAT(args, logging)\n","    if args.use_pretrain == 1:\n","        user_pre_embed = torch.tensor(data.user_pre_embed)\n","        item_pre_embed = torch.tensor(data.item_pre_embed)\n","    else:\n","        user_pre_embed, item_pre_embed = None, None\n","\n","    # construct model & optimizer\n","    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n","    #if args.use_pretrain == 2:\n","    #    model = load_model(model, args.pretrain_model_path)\n","\n","    model.to(device)\n","    #logging.info(model)\n","\n","    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","    # initialize metrics\n","    best_epoch = -1\n","    best_recall = 0\n","\n","    Ks = eval(args.Ks)\n","    k_min = min(Ks)\n","    k_max = max(Ks)\n","\n","    epoch_list = []\n","    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n","\n","    # train model\n","    for epoch in range(1, args.n_epoch + 1):\n","        time0 = time()\n","        model.train()\n","\n","        # train cf\n","        time1 = time()\n","        cf_total_loss = 0\n","        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n","\n","        for iter in range(1, n_cf_batch + 1):\n","            time2 = time()\n","            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n","            cf_batch_user = cf_batch_user.to(device)\n","            cf_batch_pos_item = cf_batch_pos_item.to(device)\n","            cf_batch_neg_item = cf_batch_neg_item.to(device)\n","\n","            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n","\n","            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n","                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n","                sys.exit()\n","\n","            cf_batch_loss.backward()\n","            cf_optimizer.step()\n","            cf_optimizer.zero_grad()\n","            cf_total_loss += cf_batch_loss.item()\n","\n","            if (iter % args.cf_print_every) == 0:\n","                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n","        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n","\n","        # train kg\n","        time3 = time()\n","        kg_total_loss = 0\n","        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n","\n","        for iter in range(1, n_kg_batch + 1):\n","            time4 = time()\n","            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n","            kg_batch_head = kg_batch_head.to(device)\n","            kg_batch_relation = kg_batch_relation.to(device)\n","            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n","            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n","\n","            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n","\n","            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n","                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n","                sys.exit()\n","\n","            kg_batch_loss.backward()\n","            kg_optimizer.step()\n","            kg_optimizer.zero_grad()\n","            kg_total_loss += kg_batch_loss.item()\n","\n","            if (iter % args.kg_print_every) == 0:\n","                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n","        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n","\n","        # update attention\n","        time5 = time()\n","        h_list = data.h_list.to(device)\n","        t_list = data.t_list.to(device)\n","        r_list = data.r_list.to(device)\n","        relations = list(data.laplacian_dict.keys())\n","        model(h_list, t_list, r_list, relations, mode='update_att')\n","        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n","\n","        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n","    return model, data, Ks, device\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"\\nComputing {'KGAT'} algorithm on Movielens {'100k'}\")\n","model, data, Ks, device, time_train = train_kgat()\n","print('time_train: ', time_train)\n","_, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n","print(metrics_dict_kgat)"]},{"cell_type":"markdown","metadata":{"id":"QsJmgwgNsBE8"},"source":["### LightGCN"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1667642261028,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"BZtl_jr3sA1o"},"outputs":[],"source":["from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n","from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n","    \n","def prepare_training_lightgcn(train, test):\n","    return ImplicitCF(train=train, test=test)\n","\n","def train_lightgcn(params, data):\n","    hparams = prepare_hparams(**params)\n","    model = LightGCN(hparams, data)\n","    with Timer() as t:\n","        model.fit()\n","    return model, t\n","\n","def recommend_k_lightgcn(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n","    with Timer() as t:\n","        topk_scores = model.recommend_k_items(\n","            test, top_k=top_k, remove_seen=remove_seen\n","        )\n","    return topk_scores, t\n","\n","lightgcn_param = {\n","    #\"yaml_file\": os.path.join(\"drive\", \"MyDrive\", \"4222Group9\", \"lightgcn.yaml\"),\n","    \"yaml_file\": \"lightgcn.yaml\",\n","    \"n_layers\": 3,\n","    \"batch_size\": 1024,\n","    \"epochs\": 10,\n","    \"learning_rate\": 0.005,\n","    \"eval_epoch\": 5,\n","    \"top_k\": DEFAULT_K,\n","    \"stacking_func\": 1, #0: original, 1: exponential decay, 1.5: exponential increase, 2: trainable with random initialization, 3: trainable with unified initialization\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare for training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1667642261029,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"WDnMUwTkmaVl"},"outputs":[],"source":["#params = {\n","#    \"als\": als_params,\n","#    \"ncf\": ncf_params,\n","#    \"lightgcn\": lightgcn_param,\n","#}\n","#prepare_training_data = {\n","#    \"als\": prepare_training_als,\n","#    \"ncf\": prepare_training_ncf,\n","#    \"lightgcn\": prepare_training_lightgcn,\n","#}\n","\n","\n","from recommenders.evaluation.spark_evaluation import (\n","    SparkRatingEvaluation,\n","    SparkRankingEvaluation,\n",")\n","def rating_metrics_pyspark(test, predictions):\n","    rating_eval = SparkRatingEvaluation(test, predictions, **COL_DICT)\n","    return {\n","        \"RMSE\": rating_eval.rmse(),\n","        \"MAE\": rating_eval.mae(),\n","        \"R2\": rating_eval.exp_var(),\n","        \"Explained Variance\": rating_eval.rsquared(),\n","    }\n","def ranking_metrics_pyspark(test, predictions, k=DEFAULT_K):\n","    rank_eval = SparkRankingEvaluation(\n","        test, predictions, k=k, relevancy_method=\"top_k\", **COL_DICT\n","    )\n","    return {\n","        \"MAP\": rank_eval.map_at_k(),\n","        \"nDCG@k\": rank_eval.ndcg_at_k(),\n","        \"Precision@k\": rank_eval.precision_at_k(),\n","        \"Recall@k\": rank_eval.recall_at_k(),\n","    }\n","\n","trainer = {\n","    \"als\": lambda params, data: train_als(params, data),\n","    \"ncf\": lambda params, data: train_ncf(params, data),\n","    \"lightgcn\": lambda params, data: train_lightgcn(params, data),\n","}\n","ranking_predictor = {\n","    \"als\": lambda model, test, train: recommend_k_als(model, test, train),\n","    \"ncf\": lambda model, test, train: recommend_k_ncf(model, test, train),\n","    \"lightgcn\": lambda model, test, train: recommend_k_lightgcn(model, test, train),\n","}\n","ranking_evaluator = {\n","    \"als\": lambda test, predictions, k: ranking_metrics_pyspark(test, predictions, k),\n","    \"ncf\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","    \"lightgcn\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","}\n","metrics = {\n","    #\"als\": [\"rating\", \"ranking\"],\n","    \"als\" : [\"ranking\"],\n","    \"ncf\": [\"ranking\"],\n","    \"lightgcn\": [\"ranking\"]\n","}"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1667642261029,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"WDnMUwTkmaVl"},"outputs":[],"source":["\n","#prepare_training_data = {\n","#    \"lightgcn\": prepare_training_lightgcn,\n","#}\n","\n","\n","from recommenders.evaluation.spark_evaluation import (\n","    SparkRatingEvaluation,\n","    SparkRankingEvaluation,\n",")\n","def rating_metrics_pyspark(test, predictions):\n","    rating_eval = SparkRatingEvaluation(test, predictions, **COL_DICT)\n","    return {\n","        \"RMSE\": rating_eval.rmse(),\n","        \"MAE\": rating_eval.mae(),\n","        \"R2\": rating_eval.exp_var(),\n","        \"Explained Variance\": rating_eval.rsquared(),\n","    }\n","def ranking_metrics_pyspark(test, predictions, k=DEFAULT_K):\n","    rank_eval = SparkRankingEvaluation(\n","        test, predictions, k=k, relevancy_method=\"top_k\", **COL_DICT\n","    )\n","    return {\n","        \"MAP\": rank_eval.map_at_k(),\n","        \"nDCG@k\": rank_eval.ndcg_at_k(),\n","        \"Precision@k\": rank_eval.precision_at_k(),\n","        \"Recall@k\": rank_eval.recall_at_k(),\n","    }\n","#prepare_metrics_data = {\n","#    \"lightgcn\": lambda train, test: prepare_metrics_lightgcn(train, test),\n","#}\n","trainer = {\n","    \"lightgcn\": lambda params, data: train_lightgcn(params, data),\n","}\n","ranking_predictor = {\n","    \"lightgcn\": lambda model, test, train: recommend_k_lightgcn(model, test, train),\n","}\n","ranking_evaluator = {\n","    \"lightgcn\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","}\n","metrics = {\n","    \"lightgcn\": [\"ranking\"]\n","}\n","params = {\n","    \"lightgcn\": lightgcn_param,\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Sanity Check by Overfitting on Small Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!pip install pytorch-lightning\n","#from pytorch_lightning import Trainer, seed_everything\n","#from LightGCN.code.main_lgcn import sanity_check\n","\n","#seed_everything(42, workers=True)\n","\n","#model = sanity_check()\n","#trainer = Trainer(max_epochs=10000, overfit_batches=0.01)\n","#trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"3B4ScYiXzCN1"},"source":["# 6 Hyperparameter Tunning"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def generate_summary(data, algo, k, train_time, time_rating, rating_metrics, time_ranking, ranking_metrics):\n","    summary = {\"Data\": data, \"Algo\": algo, \"K\": k, \"Train time (s)\": train_time, \"Predicting time (s)\": time_rating, \"Recommending time (s)\": time_ranking}\n","    if rating_metrics is None:\n","        rating_metrics = {\n","            \"RMSE\": np.nan,\n","            \"MAE\": np.nan,\n","            \"R2\": np.nan,\n","            \"Explained Variance\": np.nan,\n","        }\n","    if ranking_metrics is None:\n","        ranking_metrics = {\n","            \"MAP\": np.nan,\n","            \"nDCG@k\": np.nan,\n","            \"Precision@k\": np.nan,\n","            \"Recall@k\": np.nan,\n","        }\n","    summary.update(rating_metrics)\n","    summary.update(ranking_metrics)\n","    return summary"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","algorithms = [ \"lightgcn\"]\n","\n","lightgcn_param_dict = {\n","    #\"yaml_file\": os.path.join(\"drive\", \"MyDrive\", \"4222Group9\", \"lightgcn.yaml\"),\n","    \"yaml_file\": \"lightgcn.yaml\",\n","    \"model_type\" : 'ligntgcn',\n","    \"embed_size\" : [32, 64], # the embedding dimension of users and items\n","    \"n_layers\": 3, # number of layers of the model\n","    \"batch_size\": 1024,\n","    \"decay\" : 0.0001, # l2 regularization for embedding parameters\n","    \"epochs\": 10,\n","    \"learning_rate\": 0.005,\n","    \"eval_epoch\": 5,\n","    \"top_k\": DEFAULT_K,\n","    \"stacking_func\": 1, #0: original, 1: exponential decay, 1.5: exponential increase, 2: trainable with random initialization, 3: trainable with unified initialization\n","}"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from recommenders.tuning.parameter_sweep import generate_param_grid\n","lightgcn_params = generate_param_grid(lightgcn_param_dict)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["[{'embed_size': 32,\n","  'yaml_file': 'lightgcn.yaml',\n","  'model_type': 'ligntgcn',\n","  'n_layers': 3,\n","  'batch_size': 1024,\n","  'decay': 0.0001,\n","  'epochs': 10,\n","  'learning_rate': 0.005,\n","  'eval_epoch': 5,\n","  'top_k': 10,\n","  'stacking_func': 1},\n"," {'embed_size': 64,\n","  'yaml_file': 'lightgcn.yaml',\n","  'model_type': 'ligntgcn',\n","  'n_layers': 3,\n","  'batch_size': 1024,\n","  'decay': 0.0001,\n","  'epochs': 10,\n","  'learning_rate': 0.005,\n","  'eval_epoch': 5,\n","  'top_k': 10,\n","  'stacking_func': 1}]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["lightgcn_params"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of Movielens 100k: (100000, 4)\n","\n","Computing lightgcn algorithm on Movielens 100k\n"]},{"ename":"NameError","evalue":"name 'prepare_training_data' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:46\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'prepare_training_data' is not defined"]}],"source":["%%time\n","data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","algorithms = [ \"lightgcn\"]\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_stratified_split\n","# For each data size and each algorithm, a recommender is evaluated. \n","cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n","df_results = pd.DataFrame(columns=cols)\n","\n","for data_size in data_sizes:\n","\n","    # Load the dataset\n","    #df = movielens.load_pandas_df(\n","    #    size=data_size,\n","    #    header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n","    #)\n","    if data_size=='100k':\n","        df = pd.read_csv('ml-100k/u.data', sep='\\t', header=None)\n","\n","    df.columns = [DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n","    df[DEFAULT_RATING_COL] = df[DEFAULT_RATING_COL].astype(float)\n","    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n","    \n","    # Split the dataset\n","    df_train, df_test = python_stratified_split(df,\n","                                                ratio=0.75, \n","                                                min_rating=1, \n","                                                filter_by=\"item\", \n","                                                col_user=DEFAULT_USER_COL, \n","                                                col_item=DEFAULT_ITEM_COL,\n","                                                )\n","\n","    #df_train = \"../LightGCN/data/movielens/train.txt\"\n","    # Loop through the algos\n","    for algo in algorithms:\n","        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n","        if algo == 'kgat':\n","            model, data, Ks, device, time_train = train_kgat()\n","            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n","            print(metrics_dict_kgat)\n","            # Record results\n","            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","            #df_results.loc[df_results.shape[0] + 1] = summary\n","            \n","        else:\n","            # Data prep for training set\n","            train = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","            \n","            for model_params in lightgcn_params:\n","                print(\"training using the params:\", model_params)\n","\n","                # Train the model\n","                import traceback\n","                try:\n","                    model, time_train = trainer[algo](model_params, train)\n","                except:\n","                    traceback.print_exc()\n","                \n","                print(f\"Training time: {time_train}s\")\n","                        \n","                # Predict and evaluate\n","                #train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","                train, test = df_train, df_test\n","                \n","                if \"rating\" in metrics[algo]:   \n","                    # Predict for rating\n","                    preds, time_rating = rating_predictor[algo](model, test)\n","                    print(f\"Rating prediction time: {time_rating}s\")\n","                    \n","                    # Evaluate for rating\n","                    ratings = rating_evaluator[algo](test, preds)\n","                else:\n","                    ratings = None\n","                    time_rating = np.nan\n","                \n","                if \"ranking\" in metrics[algo]:\n","                    # Predict for ranking\n","                    top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n","                    print(f\"Ranking prediction time: {time_ranking}s\")\n","                    \n","                    # Evaluate for rating\n","                    rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n","                else:\n","                    rankings = None\n","                    time_ranking = np.nan\n","                    \n","                # Record results\n","                summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","                df_results.loc[df_results.shape[0] + 1] = summary\n","        \n","print(\"\\nComputation finished\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training Plot"]},{"cell_type":"markdown","metadata":{},"source":["click \"launch TensorBoard Session\" in main_lgcn.py"]},{"cell_type":"markdown","metadata":{},"source":["# 7 Comparisons on Movielens and Movie"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1667642261031,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"8reKYmEH2d-k"},"outputs":[],"source":["data_sizes = [\"100k\",\"1m\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","#algorithms = [ \"lightgcn\"]\n","algorithms = [\"als\", \"ncf\", \"lightgcn\", \"kgat\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1667642261031,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"-17fyWdVmiHM"},"outputs":[],"source":["%%time\n","\n","# For each data size and each algorithm, a recommender is evaluated. \n","cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n","df_results = pd.DataFrame(columns=cols)\n","\n","for data_size in data_sizes:\n","    # Load the dataset\n","    df = movielens.load_pandas_df(\n","        size=data_size,\n","        header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n","    )\n","    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n","    \n","    # Split the dataset\n","    df_train, df_test = python_stratified_split(df,\n","                                                ratio=0.75, \n","                                                min_rating=1, \n","                                                filter_by=\"item\", \n","                                                col_user=DEFAULT_USER_COL, \n","                                                col_item=DEFAULT_ITEM_COL\n","                                                )\n","   \n","    # Loop through the algos\n","    for algo in algorithms:\n","        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n","        if algo == 'kgat':\n","            model, data, Ks, device, time_train = train_kgat()\n","            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n","            print(metrics_dict_kgat)\n","            # Record results\n","            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","            #df_results.loc[df_results.shape[0] + 1] = summary\n","            \n","        else:\n","            # Data prep for training set\n","            train = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","            \n","            # Get model parameters\n","            model_params = params[algo]\n","            \n","            # Train the model\n","            model, time_train = trainer[algo](model_params, train)\n","            print(f\"Training time: {time_train}s\")\n","                    \n","            # Predict and evaluate\n","            train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","            \n","            if \"rating\" in metrics[algo]:   \n","                # Predict for rating\n","                preds, time_rating = rating_predictor[algo](model, test)\n","                print(f\"Rating prediction time: {time_rating}s\")\n","                \n","                # Evaluate for rating\n","                ratings = rating_evaluator[algo](test, preds)\n","            else:\n","                ratings = None\n","                time_rating = np.nan\n","            \n","            if \"ranking\" in metrics[algo]:\n","                # Predict for ranking\n","                top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n","                print(f\"Ranking prediction time: {time_ranking}s\")\n","                \n","                # Evaluate for rating\n","                rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n","            else:\n","                rankings = None\n","                time_ranking = np.nan\n","                \n","            # Record results\n","            summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","            df_results.loc[df_results.shape[0] + 1] = summary\n","        \n","print(\"\\nComputation finished\")"]},{"cell_type":"markdown","metadata":{},"source":["## Print the result summary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1667642261031,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"36QRfkqVrYzi"},"outputs":[],"source":["df_results"]},{"cell_type":"markdown","metadata":{"id":"seUWygXTzYfc"},"source":["# 8 Credit and Reference\n","\n","1. https://github.com/microsoft/recommenders"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python [conda env:cu101]","language":"python","name":"conda-env-cu101-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"8006cd3f4b5a31b91c77e5b5a521798c8412e8b93831dc7973f670e0263ddfcb"}}},"nbformat":4,"nbformat_minor":0}
