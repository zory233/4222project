{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vobP7ahXP1EG"
   },
   "source": [
    "# Graph-Learning-Based Recommender System on MovieLens\n",
    "\n",
    "### Group 9\n",
    "\n",
    "- AGARWAL, Sahil\n",
    "- WEI, Yuanjing\n",
    "- ZHANG, Yujun yzhanglo@connect.ust.hk\n",
    "\n",
    "Group project of COMP4222@HKUST in 2022 Fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpTqYz7qYJbP"
   },
   "source": [
    "# 1 Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yzhanglo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "executionInfo": {
     "elapsed": 30299,
     "status": "error",
     "timestamp": 1667642237984,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "MXxR7IpwI83o",
    "outputId": "9933f848-12fa-442f-afe4-0d95a20a464e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yzhanglo/4222project\n",
      "backup_main.ipynb  \u001b[0m\u001b[38;5;27mLightGCN\u001b[0m/      \u001b[38;5;27mml-latest-small\u001b[0m/  requirements.txt\n",
      "\u001b[38;5;27mcomp4222\u001b[0m/          lightgcn.yaml  movielens.ipynb\n",
      "\u001b[38;5;27mKGAT_folder\u001b[0m/       main.ipynb     \u001b[38;5;9mmovielens.zip\u001b[0m\n",
      "LICENSE            \u001b[38;5;27mml-100k\u001b[0m/       \u001b[38;5;27mrecommenders\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# change the path in the following\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd '/content/drive/MyDrive/4222Group9'\n",
    "except:\n",
    "    %cd '/data/yzhanglo/4222project'\n",
    "\n",
    "import comp4222\n",
    "import recommenders\n",
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1667642237987,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "7RuvfqktV-jm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# easier to print by putting variable as a single line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "def mask(df, key, function):\n",
    "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "  return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "  return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols\n",
    "\n",
    "# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "print(torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkgjzCmnWXUw"
   },
   "source": [
    "# 2 MovieLens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq_PrL0LZrGa"
   },
   "source": [
    "We're using ml-100k from MovieLens. It contains 100000 ratings from 943 users on 1682 movies. Each user has rated at least 20 movies. And the data was collected during the seven-month period from September 19th, 1997 through April 22nd, 1998. The readme.md is avaliable [here](https://files.grouplens.org/datasets/movielens/ml-100k-README.txt).\n",
    "\n",
    "P.s. As for now, we're using the provided code for dataset loading defined by lightgcn and microsoft recommender repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_XRmlt-yMUu"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1667642237989,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "Vt7THxaKWZDv"
   },
   "outputs": [],
   "source": [
    "# Download MovieLens data.\n",
    "dataset_name = \"ml-latest-small\"\n",
    "\n",
    "from os import path\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "if not path.exists(\"movielens.zip\"):\n",
    "    urlretrieve(f\"https://files.grouplens.org/datasets/movielens/{dataset_name}.zip\", \"movielens.zip\")\n",
    "    zipfile.ZipFile(\"movielens.zip\", \"r\").extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1667642237989,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "vhrmXmjSh5jf"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(f\"{dataset_name}/movies.csv\")\n",
    "genre_cols = [\n",
    "    \"(no genres listed)\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1667642237990,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "HipV8C2ziAUh"
   },
   "outputs": [],
   "source": [
    "tags = pd.read_csv(f\"{dataset_name}/tags.csv\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1667642237990,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "RF0KOsiadLM1"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(f\"{dataset_name}/ratings.csv\")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giSgiu7jyO04"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1667642237991,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "i5tJTHDFXmd7"
   },
   "outputs": [],
   "source": [
    "#%pip install altair\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.renderers.enable('colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIDqnYvByqGx"
   },
   "source": [
    "# 3 Preliminaries\n",
    "\n",
    "There's no much symbol to be defined at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzxFHwzLy2vU"
   },
   "source": [
    "# 4 Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f734df11e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seeds to make sure out runs are reproducible\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsJmgwgNsBE8"
   },
   "source": [
    "## LightGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning alpha_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500]], requires_grad=True)\n",
      "\u001b[0;30;43mstacking_func: 0\u001b[0m\n",
      "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
      "loading adjacency matrix\n",
      "lgn is already to go(dropout:0)\n",
      "load and save to ./LightGCN/code/checkpoints/lgn-movielens-3-64.pth.tar\n",
      "we have tensorboard!\n",
      "\u001b[0;30;43m[TEST]\u001b[0m\n",
      "{'precision': array([0.01688596, 0.01798246, 0.01758041]), 'recall': array([0.01235403, 0.02833054, 0.04183197]), 'ndcg': array([0.01855914, 0.02388089, 0.02859607])}\n",
      "EPOCH[1/10] loss0.691-|Sample:0.74|\n",
      "EPOCH[2/10] loss0.680-|Sample:1.04|\n",
      "EPOCH[3/10] loss0.626-|Sample:0.70|\n",
      "EPOCH[4/10] loss0.525-|Sample:0.68|\n",
      "EPOCH[5/10] loss0.433-|Sample:0.91|\n",
      "EPOCH[6/10] loss0.371-|Sample:0.66|\n",
      "EPOCH[7/10] loss0.336-|Sample:0.88|\n",
      "EPOCH[8/10] loss0.316-|Sample:0.60|\n",
      "EPOCH[9/10] loss0.308-|Sample:0.65|\n",
      "EPOCH[10/10] loss0.296-|Sample:1.03|\n"
     ]
    }
   ],
   "source": [
    "from LightGCN.code.main_lgcn import run_lightgcn\n",
    "# input 0: original layer-stacking weights, 1: modified layer-stacking weights\n",
    "run_lightgcn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mstacking_func: 1\u001b[0m\n",
      "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
      "loading adjacency matrix\n",
      "lgn is already to go(dropout:0)\n",
      "load and save to ../code/checkpoints/lgn-movielens-3-64.pth.tar\n",
      "\u001b[0;30;43m[TEST]\u001b[0m\n",
      "{'precision': array([0.01765351, 0.01699561, 0.01736111]), 'recall': array([0.01354759, 0.02529469, 0.03854851]), 'ndcg': array([0.02188969, 0.02450799, 0.02936132])}\n",
      "EPOCH[1/10] loss0.658-|Sample:0.82|\n",
      "EPOCH[2/10] loss0.547-|Sample:0.90|\n",
      "EPOCH[3/10] loss0.372-|Sample:0.73|\n",
      "EPOCH[4/10] loss0.297-|Sample:0.78|\n",
      "EPOCH[5/10] loss0.264-|Sample:0.95|\n",
      "EPOCH[6/10] loss0.248-|Sample:0.73|\n",
      "EPOCH[7/10] loss0.239-|Sample:0.79|\n",
      "EPOCH[8/10] loss0.223-|Sample:0.72|\n",
      "EPOCH[9/10] loss0.217-|Sample:0.79|\n",
      "EPOCH[10/10] loss0.212-|Sample:0.77|\n"
     ]
    }
   ],
   "source": [
    "from LightGCN.code.main_lgcn import run_lightgcn\n",
    "run_lightgcn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1667642237992,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "BQO0Ie-xyxut"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightGCN.code.main_lgcn import run_lightgcn\n",
    "run_lightgcn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightGCN.code.main_lgcn import run_lightgcn\n",
    "run_lightgcn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B4ScYiXzCN1"
   },
   "source": [
    "# 5 Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n",
    "algorithms = [ \"lightgcn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check by Overfitting on Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500]], requires_grad=True)\n",
      "\u001b[0;30;43mCpp extension not loaded\u001b[0m\n",
      ">>SEED: 2020\n",
      "\u001b[0;30;43mloading [LightGCN/data/movielens]\u001b[0m\n",
      "44140 interactions for training\n",
      "11235 interactions for testing\n",
      "movielens Sparsity : 0.03502087022514546\n",
      "movielens is ready to go\n",
      "===========config================\n",
      "{'A_n_fold': 100,\n",
      " 'A_split': False,\n",
      " 'alphas': Parameter containing:\n",
      "tensor([[0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500],\n",
      "        [0.2500]], requires_grad=True),\n",
      " 'bigdata': False,\n",
      " 'bpr_batch_size': 64,\n",
      " 'decay': 0.0001,\n",
      " 'dropout': 0,\n",
      " 'keep_prob': 0.6,\n",
      " 'latent_dim_rec': 64,\n",
      " 'lightGCN_n_layers': 3,\n",
      " 'lr': 0.0001,\n",
      " 'multicore': 0,\n",
      " 'pretrain': 0,\n",
      " 'stacking_func': 3,\n",
      " 'test_u_batch_size': 45}\n",
      "cores for test: 10\n",
      "comment: lgn\n",
      "tensorboard: 1\n",
      "LOAD: 0\n",
      "Weight path: ./checkpoints\n",
      "Test Topks: [20, 40, 60]\n",
      "using bpr loss\n",
      "===========end===================\n"
     ]
    }
   ],
   "source": [
    "from LightGCN.code.main_lgcn import sanity_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install pytorch-lightning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, seed_everything\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLightGCN\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain_lgcn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanity_check\n\u001b[1;32m      5\u001b[0m seed_everything(\u001b[38;5;241m42\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "#!pip install pytorch-lightning\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "model = sanity_check()\n",
    "trainer = Trainer(max_epochs=10000, overfit_batches=0.01)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n",
    "algorithms = [ \"lightgcn\"]\n",
    "%%time\n",
    "\n",
    "# For each data size and each algorithm, a recommender is evaluated. \n",
    "cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n",
    "df_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    # Load the dataset\n",
    "    df = movielens.load_pandas_df(\n",
    "        size=data_size,\n",
    "        header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n",
    "    )\n",
    "    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n",
    "    \n",
    "    # Split the dataset\n",
    "    df_train, df_test = python_stratified_split(df,\n",
    "                                                ratio=0.75, \n",
    "                                                min_rating=1, \n",
    "                                                filter_by=\"item\", \n",
    "                                                col_user=DEFAULT_USER_COL, \n",
    "                                                col_item=DEFAULT_ITEM_COL\n",
    "                                                )\n",
    "   \n",
    "    # Loop through the algos\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n",
    "        if algo == 'kgat':\n",
    "            model, data, Ks, device, time_train = train_kgat()\n",
    "            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n",
    "            print(metrics_dict_kgat)\n",
    "            # Record results\n",
    "            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            #df_results.loc[df_results.shape[0] + 1] = summary\n",
    "            \n",
    "        else:\n",
    "            # Data prep for training set\n",
    "            train = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            # Get model parameters\n",
    "            model_params = params[algo]\n",
    "            \n",
    "            # Train the model\n",
    "            model, time_train = trainer[algo](model_params, train)\n",
    "            print(f\"Training time: {time_train}s\")\n",
    "                    \n",
    "            # Predict and evaluate\n",
    "            train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            if \"rating\" in metrics[algo]:   \n",
    "                # Predict for rating\n",
    "                preds, time_rating = rating_predictor[algo](model, test)\n",
    "                print(f\"Rating prediction time: {time_rating}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                ratings = rating_evaluator[algo](test, preds)\n",
    "            else:\n",
    "                ratings = None\n",
    "                time_rating = np.nan\n",
    "            \n",
    "            if \"ranking\" in metrics[algo]:\n",
    "                # Predict for ranking\n",
    "                top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n",
    "                print(f\"Ranking prediction time: {time_ranking}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n",
    "            else:\n",
    "                rankings = None\n",
    "                time_ranking = np.nan\n",
    "                \n",
    "            # Record results\n",
    "            summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            df_results.loc[df_results.shape[0] + 1] = summary\n",
    "        \n",
    "print(\"\\nComputation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click \"launch TensorBoard Session\" in main_lgcn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Comparisons on Movielens and Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1667642261031,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "8reKYmEH2d-k"
   },
   "outputs": [],
   "source": [
    "data_sizes = [\"100k\",\"1m\"] # Movielens data size: 100k, 1m, 10m, or 20m\n",
    "#algorithms = [ \"lightgcn\"]\n",
    "algorithms = [\"als\", \"ncf\", \"lightgcn\", \"kgat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1667642261031,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "-17fyWdVmiHM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# For each data size and each algorithm, a recommender is evaluated. \n",
    "cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n",
    "df_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    # Load the dataset\n",
    "    df = movielens.load_pandas_df(\n",
    "        size=data_size,\n",
    "        header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n",
    "    )\n",
    "    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n",
    "    \n",
    "    # Split the dataset\n",
    "    df_train, df_test = python_stratified_split(df,\n",
    "                                                ratio=0.75, \n",
    "                                                min_rating=1, \n",
    "                                                filter_by=\"item\", \n",
    "                                                col_user=DEFAULT_USER_COL, \n",
    "                                                col_item=DEFAULT_ITEM_COL\n",
    "                                                )\n",
    "   \n",
    "    # Loop through the algos\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n",
    "        if algo == 'kgat':\n",
    "            model, data, Ks, device, time_train = train_kgat()\n",
    "            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n",
    "            print(metrics_dict_kgat)\n",
    "            # Record results\n",
    "            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            #df_results.loc[df_results.shape[0] + 1] = summary\n",
    "            \n",
    "        else:\n",
    "            # Data prep for training set\n",
    "            train = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            # Get model parameters\n",
    "            model_params = params[algo]\n",
    "            \n",
    "            # Train the model\n",
    "            model, time_train = trainer[algo](model_params, train)\n",
    "            print(f\"Training time: {time_train}s\")\n",
    "                    \n",
    "            # Predict and evaluate\n",
    "            train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            if \"rating\" in metrics[algo]:   \n",
    "                # Predict for rating\n",
    "                preds, time_rating = rating_predictor[algo](model, test)\n",
    "                print(f\"Rating prediction time: {time_rating}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                ratings = rating_evaluator[algo](test, preds)\n",
    "            else:\n",
    "                ratings = None\n",
    "                time_rating = np.nan\n",
    "            \n",
    "            if \"ranking\" in metrics[algo]:\n",
    "                # Predict for ranking\n",
    "                top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n",
    "                print(f\"Ranking prediction time: {time_ranking}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n",
    "            else:\n",
    "                rankings = None\n",
    "                time_ranking = np.nan\n",
    "                \n",
    "            # Record results\n",
    "            summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            df_results.loc[df_results.shape[0] + 1] = summary\n",
    "        \n",
    "print(\"\\nComputation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1667642261031,
     "user": {
      "displayName": "Yj WEI",
      "userId": "09205277183220707996"
     },
     "user_tz": -480
    },
    "id": "36QRfkqVrYzi"
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seUWygXTzYfc"
   },
   "source": [
    "# 7 Credit and Reference\n",
    "\n",
    "1. https://github.com/microsoft/recommenders"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:cute]",
   "language": "python",
   "name": "conda-env-cute-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "90fff557ab5125bf18e450eefdf7065daeca14691b1decf231ada582ea64d91c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
