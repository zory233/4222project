{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "import tensorflow as tf # NOTE: TF needs to be imported before PyTorch, otherwise we get a weird initialization error\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "import torch\n",
    "import fastai\n",
    "import surprise\n",
    "import cornac\n",
    "\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.utils.general_utils import get_number_processors\n",
    "from recommenders.utils.gpu_utils import get_cuda_version, get_cudnn_version\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_stratified_split\n",
    "from recommenders.models.fastai.fastai_utils import hide_fastai_progress_bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [\"10m\", \"20m\"] # For augmentation purposes\n",
    "data_sizes2 = [\"50k\",\"40k\",\"30k\"] # For reducing 100k dataset\n",
    "\n",
    "algorithms = [\"als\", \"svd\", \"sar\", \"ncf\", \"fastai\", \"bpr\", \"bivae\", \"lightgcn\"]\n",
    "# Main focus on LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_csv(f\"{dataset_name}/tags.csv\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(f\"{dataset_name}/ratings.csv\")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(f\"{dataset_name}/movies.csv\")\n",
    "genre_cols = [\n",
    "    \"(no genres listed)\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()\n",
    "\n",
    "#Dispaly here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "def prepare_training_als(train, test):\n",
    "    schema = StructType(\n",
    "        (\n",
    "            StructField(DEFAULT_USER_COL, IntegerType()),\n",
    "            StructField(DEFAULT_ITEM_COL, IntegerType()),\n",
    "            StructField(DEFAULT_RATING_COL, FloatType()),\n",
    "            StructField(DEFAULT_TIMESTAMP_COL, LongType()),\n",
    "        )\n",
    "    )\n",
    "    spark = start_or_get_spark()\n",
    "    return spark.createDataFrame(train, schema).cache()\n",
    "\n",
    "def prepare_metrics_als(train, test):\n",
    "    schema = StructType(\n",
    "        (\n",
    "            StructField(DEFAULT_USER_COL, IntegerType()),\n",
    "            StructField(DEFAULT_ITEM_COL, IntegerType()),\n",
    "            StructField(DEFAULT_RATING_COL, FloatType()),\n",
    "            StructField(DEFAULT_TIMESTAMP_COL, LongType()),\n",
    "        )\n",
    "    )\n",
    "    spark = start_or_get_spark()\n",
    "    return spark.createDataFrame(train, schema).cache(), spark.createDataFrame(test, schema).cache()\n",
    "\n",
    "def predict_als(model, test):\n",
    "    with Timer() as t:\n",
    "        preds = model.transform(test)\n",
    "    return preds, t\n",
    "\n",
    "def train_als(params, data):\n",
    "    symbol = ALS(**params)\n",
    "    with Timer() as t:\n",
    "        model = symbol.fit(data)\n",
    "    return model, t\n",
    "\n",
    "def recommend_k_als(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n",
    "    with Timer() as t:\n",
    "        # Get the cross join of all user-item pairs and score them.\n",
    "        users = train.select(DEFAULT_USER_COL).distinct()\n",
    "        items = train.select(DEFAULT_ITEM_COL).distinct()\n",
    "        user_item = users.crossJoin(items)\n",
    "        dfs_pred = model.transform(user_item)\n",
    "\n",
    "        # Remove seen items\n",
    "        dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "            train.alias(\"train\"),\n",
    "            (dfs_pred[DEFAULT_USER_COL] == train[DEFAULT_USER_COL])\n",
    "            & (dfs_pred[DEFAULT_ITEM_COL] == train[DEFAULT_ITEM_COL]),\n",
    "            how=\"outer\",\n",
    "        )\n",
    "        topk_scores = dfs_pred_exclude_train.filter(\n",
    "            dfs_pred_exclude_train[\"train.\" + DEFAULT_RATING_COL].isNull()\n",
    "        ).select(\n",
    "            \"pred.\" + DEFAULT_USER_COL,\n",
    "            \"pred.\" + DEFAULT_ITEM_COL,\n",
    "            \"pred.\" + DEFAULT_PREDICTION_COL,\n",
    "        )\n",
    "    return topk_scores, t\n",
    "\n",
    "\n",
    "als_params = {\n",
    "    \"rank\": 10,\n",
    "    \"maxIter\": 20,\n",
    "    \"implicitPrefs\": False,\n",
    "    \"alpha\": 0.1,\n",
    "    \"regParam\": 0.05,\n",
    "    \"coldStartStrategy\": \"drop\",\n",
    "    \"nonnegative\": False,\n",
    "    \"userCol\": DEFAULT_USER_COL,\n",
    "    \"itemCol\": DEFAULT_ITEM_COL,\n",
    "    \"ratingCol\": DEFAULT_RATING_COL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# For each data size and each algorithm, a recommender is evaluated. \n",
    "cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n",
    "df_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    # Load the dataset\n",
    "    df = movielens.load_pandas_df(\n",
    "        size=data_size,\n",
    "        header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n",
    "    )\n",
    "    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n",
    "    \n",
    "    # Split the dataset\n",
    "    df_train, df_test = python_stratified_split(df,\n",
    "                                                ratio=0.75, \n",
    "                                                min_rating=1, \n",
    "                                                filter_by=\"item\", \n",
    "                                                col_user=DEFAULT_USER_COL, \n",
    "                                                col_item=DEFAULT_ITEM_COL\n",
    "                                                )\n",
    "   \n",
    "    # Loop through the algos\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n",
    "        if algo == 'kgat':\n",
    "            model, data, Ks, device, time_train = train_kgat()\n",
    "            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n",
    "            print(metrics_dict_kgat)\n",
    "            # Record results\n",
    "            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            #df_results.loc[df_results.shape[0] + 1] = summary\n",
    "            \n",
    "        else:\n",
    "            # Data prep for training set\n",
    "            train = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            # Get model parameters\n",
    "            model_params = params[algo]\n",
    "            \n",
    "            # Train the model\n",
    "            model, time_train = trainer[algo](model_params, train)\n",
    "            print(f\"Training time: {time_train}s\")\n",
    "                    \n",
    "            # Predict and evaluate\n",
    "            train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n",
    "            \n",
    "            if \"rating\" in metrics[algo]:   \n",
    "                # Predict for rating\n",
    "                preds, time_rating = rating_predictor[algo](model, test)\n",
    "                print(f\"Rating prediction time: {time_rating}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                ratings = rating_evaluator[algo](test, preds)\n",
    "            else:\n",
    "                ratings = None\n",
    "                time_rating = np.nan\n",
    "            \n",
    "            if \"ranking\" in metrics[algo]:\n",
    "                # Predict for ranking\n",
    "                top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n",
    "                print(f\"Ranking prediction time: {time_ranking}s\")\n",
    "                \n",
    "                # Evaluate for rating\n",
    "                rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n",
    "            else:\n",
    "                rankings = None\n",
    "                time_ranking = np.nan\n",
    "                \n",
    "            # Record results\n",
    "            summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n",
    "            df_results.loc[df_results.shape[0] + 1] = summary\n",
    "        \n",
    "print(\"\\nComputation changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "recall = df_result['Recall@k']\n",
    "print(recall.min(), recall.max())\n",
    "recall = (recall - recall.min()) / (recall.max() - recall.min())\n",
    "\n",
    "fig1 = go.Scatter3d(x=df_result['embed_size'],\n",
    "                    y=df_result['learning_rate'],\n",
    "                    z=df_result['stacking_func'],\n",
    "                    text = [item for item in recall.astype(str)],\n",
    "                    marker=dict(color = recall,\n",
    "                                reversescale=False,\n",
    "                                colorscale='Blues',\n",
    "                                size=7),\n",
    "                    line=dict (width=0.02),\n",
    "                    mode='markers')\n",
    "\n",
    "#Make Plot.ly Layout\n",
    "mylayout = go.Layout(scene=dict(xaxis=dict(title=\"embed_size\"),\n",
    "                                yaxis=dict(title=\"learning_rate\"),\n",
    "                                zaxis=dict(title=\"stacking_func\")),)\n",
    "\n",
    "#Plot and save html\n",
    "plotly.offline.plot({\"data\": [fig1],\n",
    "                     \"layout\": mylayout},\n",
    "                     auto_open=True)\n",
    "# reference: https://medium.com/@prasadostwal/multi-dimension-plots-in-python-from-2d-to-6d-9a2bf7b8cc74"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
